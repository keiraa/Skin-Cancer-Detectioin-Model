{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import math\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow as tf, re, math\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "from kaggle_datasets import KaggleDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path('melanoma-512x512')\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 14\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "IMAGE_SIZE = [512, 512]\n",
    "print(BATCH_SIZE)\n",
    "print(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_path(pre):\n",
    "    return np.vectorize(lambda file: os.path.join(GCS_DS_PATH, pre, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/siim-isic-melanoma-classification/train.csv\")\n",
    "test = pd.read_csv(\"../input/siim-isic-melanoma-classification/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbc34234c10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train00-2182.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train01-2185.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train02-2193.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train03-2182.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train04-2167.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train05-2171.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train06-2175.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train07-2174.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train08-2177.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train09-2178.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train10-2174.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train11-2176.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train12-2198.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train13-2186.tfrec', 'gs://kds-df537acf009e02b1d4427274d18970a1b0101b1ca26824bae39b5b4b/train14-2174.tfrec']\n"
     ]
    }
   ],
   "source": [
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')\n",
    "\n",
    "CLASSES = [0,1] \n",
    "\n",
    "print(TRAINING_FILENAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING_FILENAMES, VALIDATION_FILENAMES = train_test_split(TRAINING_FILENAMES, test_size=0.15, random_state=907)\n",
    "\n",
    "# print(TRAINING_FILENAMES)\n",
    "# print(VALIDATION_FILENAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROT_ = 180.0\n",
    "SHR_ = 2.0\n",
    "HZOOM_ = 8.0\n",
    "WZOOM_ = 8.0\n",
    "HSHIFT_ = 8.0\n",
    "WSHIFT_ = 8.0\n",
    "\n",
    "dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear    = math.pi * shear    / 180.\n",
    "\n",
    "    def get_3x3_mat(lst):\n",
    "        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1   = tf.math.cos(rotation)\n",
    "    s1   = tf.math.sin(rotation)\n",
    "    one  = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    \n",
    "    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n",
    "                                   -s1,  c1,   zero, \n",
    "                                   zero, zero, one])    \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)    \n",
    "    \n",
    "    shear_matrix = get_3x3_mat([one,  s2,   zero, \n",
    "                                zero, c2,   zero, \n",
    "                                zero, zero, one])        \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n",
    "                               zero,            one/width_zoom, zero, \n",
    "                               zero,            zero,           one])    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n",
    "                                zero, one,  width_shift, \n",
    "                                zero, zero, one])\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), \n",
    "                 K.dot(zoom_matrix,     shift_matrix))\n",
    "\n",
    "\n",
    "def transform(image, DIM=512):    \n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    rot = ROT_ * tf.random.normal([1], dtype='float32')\n",
    "    shr = SHR_ * tf.random.normal([1], dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\n",
    "    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\n",
    "    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n",
    "    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n",
    "\n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n",
    "    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n",
    "    z   = tf.ones([DIM*DIM], dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n",
    "    idx2 = K.cast(idx2, dtype='int32')\n",
    "    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n",
    "    d    = tf.gather_nd(image, tf.transpose(idx3))\n",
    "        \n",
    "    return tf.reshape(d,[DIM, DIM,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotate(x: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Rotation augmentation\n",
    "\n",
    "    Args:\n",
    "        x: Image\n",
    "\n",
    "    Returns:\n",
    "        Augmented image\n",
    "    \"\"\"\n",
    "\n",
    "    # Rotate 0, 90, 180, 270 degrees\n",
    "    return tf.image.rot90(x, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_central_crop(x: tf.Tensor) -> tf.Tensor:\n",
    "    x = tf.image.central_crop(x, random.uniform(0.9,1.0))\n",
    "    x = tf.image.resize(x, IMAGE_SIZE)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 32692 training images, 10982 unlabeled test images\n"
     ]
    }
   ],
   "source": [
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        #\"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    #label = tf.cast(example['class'], tf.int32)\n",
    "    label = tf.cast(example['target'], tf.int32)\n",
    "    return image, label # returns a dataset of (image, label) pairs\n",
    "\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
    "        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    idnum = example['image_name']\n",
    "    return image, idnum # returns a dataset of image(s)\n",
    "\n",
    "def aug_image(image, aug):\n",
    "    if '0' in aug:\n",
    "        image = image\n",
    "    if '1' in aug:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "    if '2' in aug:\n",
    "        image = tf.image.flip_up_down(image)\n",
    "    if '3' in aug:\n",
    "        image = tf.image.rot90(image, 1)\n",
    "    if '4' in aug:\n",
    "        image = tf.image.rot90(image, 2)\n",
    "    if '5' in aug:\n",
    "        image = tf.image.rot90(image, 3) \n",
    "    return image\n",
    "    \n",
    "    \n",
    "def read_unlabeled_test_tfrecord(example, aug):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
    "        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    image = aug_image(image, aug)\n",
    "    idnum = example['image_name']\n",
    "    return image, idnum # returns a dataset of image(s)\n",
    "\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset\n",
    "\n",
    "def load_test_dataset(aug, filenames, labeled=False, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(lambda x: read_unlabeled_test_tfrecord(x, aug), num_parallel_calls=AUTO)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset\n",
    "\n",
    "def data_augment(image, label):\n",
    "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
    "    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
    "    # of the TPU while the TPU itself is computing gradients.\n",
    "    image = transform(image,DIM=dim)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    #image = random_rotate(image)\n",
    "    #image = random_central_crop(image)\n",
    "    #image = tf.image.random_hue(image, 0.01)\n",
    "    image = tf.image.random_saturation(image, 0.7, 1.25)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "    image = tf.image.random_brightness(image, 0.1)\n",
    "    return image, label   \n",
    "\n",
    "def get_training_dataset():\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
    "    \n",
    "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(ordered=False):\n",
    "    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(aug, ordered=False):\n",
    "    dataset = load_test_dataset(aug, TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "# NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\n",
    "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
    "\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "print('Dataset: {} training images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_TEST_IMAGES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "        return 2e-4 * 0.5**epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_auc', factor=0.4, patience=1, verbose=1, mode='max'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_auc', mode = 'max', patience = 4, \n",
    "                                                      verbose = 1, min_delta = 0.0001, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_noisy-student_notop.h5\n",
      "71680000/71678424 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    base_model = efn.EfficientNetB4(\n",
    "            input_shape=(*IMAGE_SIZE, 3),\n",
    "            weights='noisy-student',\n",
    "            include_top=False\n",
    "        )\n",
    "    auc = tf.keras.metrics.AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b4 (Model)      (None, 16, 16, 1792)      17673816  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1793      \n",
      "=================================================================\n",
      "Total params: 17,675,609\n",
      "Trainable params: 17,550,409\n",
      "Non-trainable params: 125,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        L.GlobalAveragePooling2D(),  \n",
    "        L.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    \n",
    "model.compile(\n",
    "    optimizer=adam,\n",
    "    #loss = 'binary_crossentropy',\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.2),\n",
    "    metrics=['binary_crossentropy', auc]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'efficientnetb4.h5', monitor='val_auc', verbose=1, save_best_only=True,\n",
    "    save_weights_only=True, mode='auto', save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 1/14\n",
      "255/255 [==============================] - 125s 491ms/step - binary_crossentropy: 0.1627 - auc: 0.7015 - loss: 0.3606 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 2/14\n",
      "255/255 [==============================] - 119s 466ms/step - binary_crossentropy: 0.1516 - auc: 0.8236 - loss: 0.3528 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 5e-05.\n",
      "Epoch 3/14\n",
      "255/255 [==============================] - 120s 469ms/step - binary_crossentropy: 0.1497 - auc: 0.8726 - loss: 0.3515 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 2.5e-05.\n",
      "Epoch 4/14\n",
      "255/255 [==============================] - 120s 472ms/step - binary_crossentropy: 0.1490 - auc: 0.8938 - loss: 0.3510 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 1.25e-05.\n",
      "Epoch 5/14\n",
      "255/255 [==============================] - 118s 463ms/step - binary_crossentropy: 0.1478 - auc: 0.8876 - loss: 0.3503 - lr: 1.2500e-05\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 6.25e-06.\n",
      "Epoch 6/14\n",
      "255/255 [==============================] - 120s 471ms/step - binary_crossentropy: 0.1469 - auc: 0.9007 - loss: 0.3496 - lr: 6.2500e-06\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 3.125e-06.\n",
      "Epoch 7/14\n",
      "255/255 [==============================] - 119s 466ms/step - binary_crossentropy: 0.1474 - auc: 0.8883 - loss: 0.3500 - lr: 3.1250e-06\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 1.5625e-06.\n",
      "Epoch 8/14\n",
      "255/255 [==============================] - 121s 473ms/step - binary_crossentropy: 0.1469 - auc: 0.9006 - loss: 0.3499 - lr: 1.5625e-06\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 7.8125e-07.\n",
      "Epoch 9/14\n",
      "255/255 [==============================] - 121s 473ms/step - binary_crossentropy: 0.1473 - auc: 0.8953 - loss: 0.3500 - lr: 7.8125e-07\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 3.90625e-07.\n",
      "Epoch 10/14\n",
      "255/255 [==============================] - 120s 471ms/step - binary_crossentropy: 0.1465 - auc: 0.9033 - loss: 0.3494 - lr: 3.9062e-07\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 1.953125e-07.\n",
      "Epoch 11/14\n",
      "255/255 [==============================] - 118s 464ms/step - binary_crossentropy: 0.1470 - auc: 0.8968 - loss: 0.3497 - lr: 1.9531e-07\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 9.765625e-08.\n",
      "Epoch 12/14\n",
      "255/255 [==============================] - 118s 465ms/step - binary_crossentropy: 0.1467 - auc: 0.9068 - loss: 0.3496 - lr: 9.7656e-08\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 4.8828125e-08.\n",
      "Epoch 13/14\n",
      "255/255 [==============================] - 119s 465ms/step - binary_crossentropy: 0.1469 - auc: 0.9046 - loss: 0.3498 - lr: 4.8828e-08\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 2.44140625e-08.\n",
      "Epoch 14/14\n",
      "255/255 [==============================] - 121s 473ms/step - binary_crossentropy: 0.1465 - auc: 0.9008 - loss: 0.3494 - lr: 2.4414e-08\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    get_training_dataset(), \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[lr_schedule],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    #validation_data=get_validation_dataset(ordered=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('efficientnetb4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('efficientnetb5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_ls = [0,1,2,3,4,5,12,13,14,15,23,24,25,26,123,124,125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aug_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10033005]\n",
      " [0.10440153]\n",
      " [0.11435565]\n",
      " ...\n",
      " [0.0956023 ]\n",
      " [0.11091182]\n",
      " [0.10806203]]\n",
      "[[0.10361144]\n",
      " [0.10305351]\n",
      " [0.11086151]\n",
      " ...\n",
      " [0.09715959]\n",
      " [0.11932749]\n",
      " [0.11046839]]\n",
      "[[0.10606766]\n",
      " [0.1052717 ]\n",
      " [0.10557625]\n",
      " ...\n",
      " [0.10535747]\n",
      " [0.11853519]\n",
      " [0.10635233]]\n",
      "[[0.10435671]\n",
      " [0.10610953]\n",
      " [0.10341346]\n",
      " ...\n",
      " [0.10241339]\n",
      " [0.11659196]\n",
      " [0.10890555]]\n",
      "[[0.10338008]\n",
      " [0.10161638]\n",
      " [0.1178014 ]\n",
      " ...\n",
      " [0.10307989]\n",
      " [0.11412758]\n",
      " [0.11169538]]\n",
      "[[0.10779804]\n",
      " [0.10807586]\n",
      " [0.10547367]\n",
      " ...\n",
      " [0.09936321]\n",
      " [0.12184787]\n",
      " [0.09979793]]\n",
      "[[0.10338008]\n",
      " [0.10161638]\n",
      " [0.1178014 ]\n",
      " ...\n",
      " [0.10307989]\n",
      " [0.11412758]\n",
      " [0.11169538]]\n",
      "[[0.095945  ]\n",
      " [0.10553601]\n",
      " [0.10954311]\n",
      " ...\n",
      " [0.10676375]\n",
      " [0.11184078]\n",
      " [0.11207768]]\n",
      "[[0.10606766]\n",
      " [0.1052717 ]\n",
      " [0.10557625]\n",
      " ...\n",
      " [0.10535747]\n",
      " [0.11853519]\n",
      " [0.10635233]]\n",
      "[[0.10720113]\n",
      " [0.1042507 ]\n",
      " [0.10121846]\n",
      " ...\n",
      " [0.10940149]\n",
      " [0.10569918]\n",
      " [0.10525545]]\n",
      "[[0.10720113]\n",
      " [0.1042507 ]\n",
      " [0.10121846]\n",
      " ...\n",
      " [0.10940149]\n",
      " [0.10569918]\n",
      " [0.10525545]]\n",
      "[[0.10361144]\n",
      " [0.10305351]\n",
      " [0.11086151]\n",
      " ...\n",
      " [0.09715959]\n",
      " [0.11932749]\n",
      " [0.11046839]]\n",
      "[[0.095945  ]\n",
      " [0.10553601]\n",
      " [0.10954311]\n",
      " ...\n",
      " [0.10676375]\n",
      " [0.11184078]\n",
      " [0.11207768]]\n",
      "[[0.10606766]\n",
      " [0.1052717 ]\n",
      " [0.10557625]\n",
      " ...\n",
      " [0.10535747]\n",
      " [0.11853519]\n",
      " [0.10635233]]\n",
      "[[0.10779804]\n",
      " [0.10807586]\n",
      " [0.10547367]\n",
      " ...\n",
      " [0.09936321]\n",
      " [0.12184787]\n",
      " [0.09979793]]\n",
      "[[0.10033005]\n",
      " [0.10440153]\n",
      " [0.11435565]\n",
      " ...\n",
      " [0.0956023 ]\n",
      " [0.11091182]\n",
      " [0.10806203]]\n",
      "[[0.10435671]\n",
      " [0.10610953]\n",
      " [0.10341346]\n",
      " ...\n",
      " [0.10241339]\n",
      " [0.11659196]\n",
      " [0.10890555]]\n"
     ]
    }
   ],
   "source": [
    "for i in aug_ls:\n",
    "    test_ds = get_test_dataset(str(i), ordered=True)\n",
    "\n",
    "    test_images_ds = test_ds.map(lambda image, idnum: image)\n",
    "    probs = model.predict(test_images_ds)\n",
    "    try:\n",
    "        probabilities += probs\n",
    "    except:\n",
    "        probabilities = probs      \n",
    "    print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = probabilities/len(aug_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating submission.csv file...\n"
     ]
    }
   ],
   "source": [
    "print('Generating submission.csv file...')\n",
    "test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n",
    "test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_2272503</td>\n",
       "      <td>0.103732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_9821912</td>\n",
       "      <td>0.104818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_4347483</td>\n",
       "      <td>0.108357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_6645719</td>\n",
       "      <td>0.105522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0456797</td>\n",
       "      <td>0.098440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name    target\n",
       "0  ISIC_2272503  0.103732\n",
       "1  ISIC_9821912  0.104818\n",
       "2  ISIC_4347483  0.108357\n",
       "3  ISIC_6645719  0.105522\n",
       "4  ISIC_0456797  0.098440"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame({'image_name': test_ids, 'target': np.concatenate(probabilities)})\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0052060</td>\n",
       "      <td>0.092760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0052349</td>\n",
       "      <td>0.097089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0058510</td>\n",
       "      <td>0.097423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0073313</td>\n",
       "      <td>0.106177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0073502</td>\n",
       "      <td>0.123713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name    target\n",
       "0  ISIC_0052060  0.092760\n",
       "1  ISIC_0052349  0.097089\n",
       "2  ISIC_0058510  0.097423\n",
       "3  ISIC_0073313  0.106177\n",
       "4  ISIC_0073502  0.123713"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del sub['target']\n",
    "sub = sub.merge(pred_df, on='image_name')\n",
    "sub.to_csv('efficientnetb4.csv', index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
